{
 cells [
  {
   cell_type markdown,
   source [
    # Embeddings Exercisen,
    ## Preparationn,
    - 1 Download Word2Vec embeddings from [link](httpwww.italianlp.itresourcesitalian-word-embeddings), both are in sqlite file format. Each embedding has 128 dimensions.n,
    - 2 Get the [Haspeede 2020](httpsceur-ws.orgVol-2765paper162.pdf) datasetn,
    - 3 Install pandas, sqlite3 and scikit-learn libraries.n,
    n,
    ## Exercisen,
    Use word embeddings from point 1 to classify hate speech from dataset at point 2. The Haspedee dataset contains Twitter data labeled with Hate or No-Hate.n
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # Insert here paths to embeddings filesn,
    itwac_path = 'dataword_2_vecitwac128.sqlite'n,
    twitter_path ='dataword_2_vectwitter128.sqlite'n,
    n,
    # Insert path to haspedee datasets heren,
    haspedee_dataset_path = 'hate_speechhaspeede2020haspeede2_dev_taskAB.tsv'n,
    n,
    data_path = twitter_path
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # Import and initializationsn,
    import pandas as pdn,
    import sqlite3n,
    import randomn,
    from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_scoren,
    n,
    id_to_label = {0NoHate, 1 Hate}n,
    text_to_id_map = {}n,
    n,
    def read_embedings(sqllite_path)n,
         Read sqlite embeddings from sqllite_path and returns them into a pandas DataFramen,
        n,
        con = sqlite3.connect(sqllite_path)n,
        df = pd.read_sql_query(SELECT  FROM store, con)n,
        con.close()n,
        return  dfn,
    n,
    def read_dataset(input_file)n,
      examples = []n,
      labels = []n,
      with open(input_file, 'r', encoding='utf-8', errors='ignore') as fn,
          contents = f.read()n,
          file_as_list = contents.splitlines()n,
          random.shuffle(file_as_list)n,
          for line in file_as_listn,
              if line.startswith(id)n,
                continuen,
              split = line.split(t)n,
              text = split[1]n,
              label = split[2]n,
              text_to_id_map[text] = split[0]n,
              labels.append(label)n,
              examples.append(text)n,
          f.close()n,
      return examples, labelsn
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # Reading embeddingsn,
    # Each row contains a word and the corresponding embedding (128 dimensions) n,
    df = read_embedings(data_path)
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # x contains the textual datasetn,
    # y_ref contains the reference label i.e., 0 for no-hate and 1 for hate-n,
    x, y_ref = read_dataset(haspedee_dataset_path)
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # PUT HERE YOUR CODEn,
    n,
    n,
    n,
    y_hyp = None
   ],
   metadata {
    collapsed false
   }
  },
  {
   cell_type code,
   execution_count null,
   outputs [],
   source [
    # Evaluate your results with these metricsn,
    a = accuracy_score(y_ref,y_hyp)n,
    p = precision_score(y_ref, y_hyp, pos_label=1)n,
    r = recall_score(y_ref, y_hyp, pos_label=1)n,
    f1 = f1_score(y_ref, y_hyp, pos_label=1)n,
    print(precision  + str(p) )n,
    print(recall  + str(r) )n,
    print(accuracy  + str(a) )n,
    print(f1  + str(f1) )
   ],
   metadata {
    collapsed false
   }
  }
 ],
 metadata {
  kernelspec {
   display_name Python 3,
   language python,
   name python3
  },
  language_info {
   codemirror_mode {
    name ipython,
    version 2
   },
   file_extension .py,
   mimetype textx-python,
   name python,
   nbconvert_exporter python,
   pygments_lexer ipython2,
   version 2.7.6
  }
 },
 nbformat 4,
 nbformat_minor 0
}
